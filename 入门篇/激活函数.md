```python
"""Introduction of common activation function ,the following
fuctions all depend on a batch of inputed data instead of a single one."""
import numpy as np
import matplotlib.pyplot as plt


def sigmoid(x):
    x = np.array(x)
    return 1.0 / (1 + np.exp(-x))


def step_function(x):
    x = np.array(x)
    y = x > 0
    return y.astype('int')


def Relu_key1(x):
    x = np.array(x)
    x[x <= 0] = 0
    return x


def Relu_key2(x):
    x = np.array(x)
    return np.maximum(0, x)

    
a = np.linspace(-10, 10, 100)
plt.plot(a, sigmoid(a))
plt.show()
"""The functions showed above are usually used in the hidden layer, as for the output layer, the below functions are common"""


# The following function presents an overflow problem, where the number may be too large for the computer to represent 
def softmax(x):
    x = np.array(x)
    exp_x = np.exp(x)
    sum_exp_x = np.sum(exp_x)
    y = exp_x / sum_exp_x
    return y


# now let's see the upgrated version:
# this version can solve the problem occured on the softmax(),because there are some changes in the upgrade_sotfmax(),
# which can be proved to equales to the original one
def upgrade_softmax(x):
    x = np.array(x)
    c = np.max(x)
    exp_x = np.exp(x - c)
    sum_exp_x = np.sum(exp_x)
    y = exp_x / sum_exp_x
    return y


def identity_function(x):
    return np.array(x)


"""The output of softmax is probability, and the sum of all outputs is equal to 1, but this function is computationally intensive.So the identity_function is often used even though its outputs is not probability."""
```