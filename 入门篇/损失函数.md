```python
"""The introduction of the common loss function"""
import numpy as np


# 均方误差
def mean_squar_error(x, labels):
    x = np.array(x)
    labels = np.array(labels)
    return 0.5 * np.sum(2 ** (x - labels))


# 交叉熵误差(when the labels are expressed in the one-hot way)
def cross_entropy_error_one_hot_labels(x, labels):
    x = np.array(x)
    # Prevent the data from being 0 so that the later training cannot be carried out
    delta = 1e-7 
    labels = np.array(labels)
    return np.sum(-labels * np.log(x + delta))


# 交叉熵误差(when the data transfered into the function is mini_batch)
def mini_cross_entropy_error(x, labels):
    x = np.array(x)
    if x.ndim == 1:
        labels = labels.reshape((1, -1))
        x = x.reshape((1, -1))
    batch_size = x.shape[0]
    return np.sum(-labels * np.log(x + 1e-7)) / batch_size


# when the labels are not expressed in one-hot way:
def mini_cross_entropy_error_none_hot(x, labels):
    x = np.array(x)
    labels = np.array(labels)
    if x.ndim == 1:
        x = x.reshape((1, -1))
        labels = labels.reshape((1, -1))
    batch_size = x.shape[0]
    return (-np.sum(np.log(x[np.arange(batch_size), labels] + 1e-7)) / batch_size)
    
```